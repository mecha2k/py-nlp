{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Install\n",
    "\n",
    "\n",
    "## Using Make\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/facebookresearch/fastText.git\n",
    "$ cd fastText\n",
    "$ mkdir build && cd build && cmake ..\n",
    "$ make && make install\n",
    "```\n",
    "\n",
    "## Using Pip\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/facebookresearch/fastText.git\n",
    "$ cd fastText\n",
    "$ pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouble-shooting\n",
    "\n",
    "### In Mac\n",
    "\n",
    "In case if you have failure to install 'fastText', install following packages.\n",
    "\n",
    "```bash\n",
    "$ xcode-select --install\n",
    "$ brew install cmake\n",
    "$ brew istall gcc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fasttext <command> <args>\r\n",
      "\r\n",
      "The commands supported by fasttext are:\r\n",
      "\r\n",
      "  supervised              train a supervised classifier\r\n",
      "  quantize                quantize a model to reduce the memory usage\r\n",
      "  test                    evaluate a supervised classifier\r\n",
      "  test-label              print labels with precision and recall scores\r\n",
      "  predict                 predict most likely labels\r\n",
      "  predict-prob            predict most likely labels with probabilities\r\n",
      "  skipgram                train a skipgram model\r\n",
      "  cbow                    train a cbow model\r\n",
      "  print-word-vectors      print word vectors given a trained model\r\n",
      "  print-sentence-vectors  print sentence vectors given a trained model\r\n",
      "  print-ngrams            print ngrams given a trained model and word\r\n",
      "  nn                      query for nearest neighbors\r\n",
      "  analogies               query for analogies\r\n",
      "  dump                    dump arguments,dictionary,input/output vectors\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래서 두 번 째 더 큰 문제 를 야기 하 죠 . 그것 은 바로 자선 단체 들 이 간접비 를 낮추 는 것 에 만 몰두 하 여 성장 에 꼭 필요 한 비용 의 지출 없이 운영 되 게 한다는 것 입니다 . \r\n",
      "감사 합니다 . \r\n",
      "오비완 케노비 와 글 린다 그 둘 의 공통점 은 무엇 일까요 ? \r\n",
      "하지만 , 진실 은 자연 은 죽음 에 능숙 하 다는 것 입니다 . \r\n",
      "이 후보 는 페르시아 어로 \" 불멸 \" 을 뜻 한다고 합니다 . \r\n",
      "그 이유 는 바로 - 바로 잠깐 만 요 . \r\n",
      "그 사건 을 겪 었 을 때 , 말 그대로 , 우리 의 삶 에 정말 끔찍 한 충격 이 었 음 에 도 불구 하 고 , 우리 는 복수심 을 가지 지 않 았 습니다 . \r\n",
      "하지만 우리 애 들 같 은 애 들 은 애시당초 선물 로 주어진 운명 이 아니 에요 . \r\n",
      "소위 신경 - 조율기 를 만드 는 회사 를 차려서 , 간질 뿐 만 아니 라 다른 뇌 질환 들 을 치료 하 고자 한 거 죠 , 왜냐하면 비록 전부 는 아니 라고 해도 많 은 수 의 뇌 질환 들 이 어떠 한 전기 적 인 기능 이상 에서 비롯 된 것 이 기 때문 입니다 . \r\n",
      "좋 아요 , 이제 이걸 켜 보 죠 . \r\n"
     ]
    }
   ],
   "source": [
    "!head ./ted.aligned.ko.refined.tok.random-10k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  3589\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7514 lr:  0.000000 loss:  1.878789 ETA:   0h 0m ETA:   0h 3m2.808293 ETA:   0h 3m loss:  2.763790 ETA:   0h 3m 2.761820 ETA:   0h 3m ETA:   0h 3m loss:  2.627547 ETA:   0h 3m0.046981 loss:  2.615879 ETA:   0h 3m0.046068 loss:  2.561978 ETA:   0h 3m loss:  2.283066 ETA:   0h 3m2.275010 ETA:   0h 3m0.038445 loss:  2.238237 ETA:   0h 3m 2.214851 ETA:   0h 3m 0.036131 loss:  2.187801 ETA:   0h 3mm  0h 3m 2.182410 ETA:   0h 3m0.035416 loss:  2.175688 ETA:   0h 3m0.035382 loss:  2.175174 ETA:   0h 3m loss:  2.167780 ETA:   0h 3m2.162519 ETA:   0h 3m 2.153429 ETA:   0h 3m 2.140303 ETA:   0h 3m2.129294 ETA:   0h 3m  0h 3m 2.124553 ETA:   0h 3m 3m ETA:   0h 2m 2.090912 ETA:   0h 2mm lr:  0.025737 loss:  2.050997 ETA:   0h 2m loss:  2.046522 ETA:   0h 2m% words/sec/thread:    7712 lr:  0.025052 loss:  2.044019 ETA:   0h 2m 2.041097 ETA:   0h 2m0.024393 loss:  2.038524 ETA:   0h 2m2.035597 ETA:   0h 2m 2.022197 ETA:   0h 2mm loss:  2.010890 ETA:   0h 2m 2.009184 ETA:   0h 2m 0.020118 loss:  2.002101 ETA:   0h 1m ETA:   0h 1m 1.998121 ETA:   0h 1m loss:  1.989830 ETA:   0h 1m 1.987859 ETA:   0h 1m 1.986883 ETA:   0h 1m0.017370 loss:  1.982062 ETA:   0h 1m 0.017352 loss:  1.981942 ETA:   0h 1m 1.980309 ETA:   0h 1m1.978559 ETA:   0h 1m0.016820 loss:  1.978347 ETA:   0h 1m0.015948 loss:  1.971728 ETA:   0h 1m lr:  0.015832 loss:  1.970685 ETA:   0h 1m 1.958985 ETA:   0h 1m 1.957507 ETA:   0h 1m 1.954864 ETA:   0h 1m 1.950421 ETA:   0h 1m 0.011504 loss:  1.943313 ETA:   0h 1m 0.010840 loss:  1.938818 ETA:   0h 1m ETA:   0h 0m  0h 0m 1.921993 ETA:   0h 0m loss:  1.921897 ETA:   0h 0m ETA:   0h 0m  0h 0m  0h 0m 1.903354 ETA:   0h 0m 0.002959 loss:  1.893776 ETA:   0h 0m  0h 0m\n"
     ]
    }
   ],
   "source": [
    "!fasttext skipgram -input ./ted.aligned.ko.refined.tok.random-10k.txt -output ko.tok -dim 256 -epoch 100 -minCount 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word? 해결 0.545583\r\n",
      "중대 0.319438\r\n",
      "이민 0.290259\r\n",
      "짚 0.274612\r\n",
      "에 0.269835\r\n",
      "고민 0.266738\r\n",
      "우리 0.266326\r\n",
      "보건 0.265246\r\n",
      "직면 0.259586\r\n",
      "이 0.257129\r\n",
      "Query word? "
     ]
    }
   ],
   "source": [
    "!echo '문제' | fasttext nn ./ko.tok.bin 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589 256\r\n",
      ". 0.043397 -0.13702 -0.018087 -0.084497 -0.077254 0.0091988 0.18928 -0.026841 0.02685 0.047803 -0.23921 -0.056236 0.12254 0.020359 -0.0061015 -0.032616 -0.01439 -0.065073 -0.1111 0.014269 -0.091981 0.020357 -0.0082178 -0.12498 0.13287 0.19074 0.068208 0.026437 -0.052568 0.038764 -0.0064421 0.076457 -0.014887 -0.18889 0.11472 0.067244 -0.11793 -0.19888 0.086999 -0.052266 0.033103 0.032948 -0.0691 -0.042505 -0.14346 -0.15657 0.14691 -0.086014 0.054112 -0.10866 -0.086563 -0.02743 -0.041374 -0.029774 0.048363 -0.19961 -0.1137 -0.024816 0.14863 -0.24343 -0.056697 0.089899 0.038359 0.19129 0.01418 -0.010353 0.031443 0.099836 0.12471 0.046054 0.027525 -0.057544 -0.0053051 0.090709 -0.064621 0.011738 -0.025581 -0.011543 -0.062408 0.036848 -0.0089185 0.13916 0.015261 0.0062368 0.057514 -0.010355 -0.13965 -0.058341 0.0021778 -0.21925 -0.060655 -0.12518 0.13915 0.11512 0.078169 -0.077675 0.14177 -0.048355 -0.012878 0.042037 0.05887 -0.069756 0.094365 0.18508 -0.063475 -0.022312 -0.070417 -0.065491 0.025424 0.17381 0.026143 -0.011645 -0.1058 0.19435 0.028964 -0.049977 0.080622 -0.047906 0.097413 -0.0023848 0.052081 0.017234 0.088821 0.036302 0.14613 0.050144 -0.063771 -0.021188 -0.012161 0.0052707 0.066839 -0.029836 -0.011128 -0.081044 0.069318 0.051337 0.096491 -0.092953 0.02546 0.054259 0.070516 -0.010492 -0.15121 0.014006 -0.012263 0.061741 0.17476 -0.080034 0.13911 0.045463 0.026474 -0.060951 -0.021984 0.046871 -0.0051539 -0.056899 -0.013022 0.081792 0.016493 -0.14037 0.00055137 -0.057557 0.061685 -0.012286 0.11737 -0.014617 -0.026291 0.013191 0.033834 0.041026 -0.078764 0.089435 0.024249 0.024812 -0.036022 0.0075935 -0.023388 0.16605 0.029478 0.013369 0.10918 -0.0069197 0.038116 0.072406 0.063364 -0.02258 -0.067285 -0.037065 -0.10667 -0.049511 0.08951 -0.031525 -0.055899 0.04384 -0.013004 0.090925 0.15981 -0.17714 0.02922 -0.014511 -0.069542 0.18139 0.1909 -0.088049 0.15185 0.02283 -0.041718 0.11985 -0.02359 -0.023613 -0.083701 0.0032343 -0.10812 -0.042357 0.12803 -0.019946 -0.082564 -0.1205 0.030463 0.029347 0.06891 0.044089 0.15807 0.079908 -0.037481 -0.037687 0.014122 -0.0068938 -0.038741 0.092697 -0.048157 0.096898 -0.011889 0.11646 0.061933 0.1728 0.055275 0.06138 -0.02409 0.023166 -0.045862 0.12286 0.050321 0.043042 -0.058517 0.085625 0.027052 0.15071 0.082374 -0.091169 0.17876 -0.023577 0.10748 0.046252 -0.13111 -0.053302 \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 ./ko.tok.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
